{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import shap\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scripts.utils import log_msg, precision_reall_f1_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={}\n",
    "args['data'] = 'data/sample_data_features.csv'\n",
    "args['feature_space'] = 'data/feature_names.csv'\n",
    "args['test_size'] = 0.2\n",
    "args['seed'] = 123456\n",
    "\n",
    "args['cv_results'] = 'results/cv_lightgbm.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['NFOLDS'] = 3\n",
    "args['num_iterations'] = [20, 40, 60, 80, 100]\n",
    "args['early_stopping_rounds'] = 400\n",
    "args['verbose'] = 0\n",
    "args['n_jobs'] = 4\n",
    "args['verbose_eval'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(args['data'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(['sequence', 'label'], axis=1), \n",
    "                                                    data[['label']], \n",
    "                                                    test_size=args['test_size'], \n",
    "                                                    random_state=args['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=args['NFOLDS'], shuffle=True, random_state=args['seed'])\n",
    "\n",
    "columns = X_train.columns\n",
    "score = 0\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = columns\n",
    "\n",
    "results = []\n",
    "log_msg(\">>> Start CV\")\n",
    "for i in range(len(args['num_iterations'])):\n",
    "\n",
    "    gc.collect()\n",
    "    score = 0\n",
    "    splits = folds.split(X_train, y_train)\n",
    "    \n",
    "    params = {'objective':'binary',\n",
    "              'num_iterations': args['num_iterations'][i],\n",
    "              'n_jobs': args['n_jobs'],\n",
    "              'random_state': args['seed'],\n",
    "              \"metric\": 'auc',\n",
    "              'verbosity': args['verbose']\n",
    "         }\n",
    "\n",
    "    for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "\n",
    "        X_train_cv, X_valid = X_train[columns].iloc[train_index], X_train[columns].iloc[valid_index]\n",
    "        y_train_cv, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "        dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n",
    "        dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "        clf = lgb.train(params, dtrain, valid_sets = [dvalid], verbose_eval=args['verbose_eval'], \n",
    "                        early_stopping_rounds=args['early_stopping_rounds'])\n",
    "\n",
    "        y_pred_valid = clf.predict(X_valid)\n",
    "\n",
    "        score += roc_auc_score(y_valid, y_pred_valid) / args['NFOLDS']\n",
    "        \n",
    "        del X_train_cv, X_valid, y_train_cv, y_valid\n",
    "        del clf, y_pred_valid\n",
    "\n",
    "    log_msg(f\">>> num_iterations = {args['num_iterations'][i]} Mean AUC = {score}\")\n",
    "    \n",
    "    results.append({'num_iterations': args['num_iterations'][i],\n",
    "               'score': score})\n",
    "    \n",
    "log_msg(\">>> Finished!\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final, X_valid, y_train_final, y_valid = train_test_split(X_train, y_train, \n",
    "                                                                  test_size=0.1, random_state=args['seed'])\n",
    "\n",
    "params = {'objective':'binary',\n",
    "          'num_iterations': 100,\n",
    "          'n_jobs': 4,\n",
    "          'random_state': args['seed'],\n",
    "          \"metric\": 'auc',\n",
    "          'verbosity': args['verbose']\n",
    "         }\n",
    "\n",
    "dtrain = lgb.Dataset(X_train_final, label=y_train_final)\n",
    "dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "clf = lgb.train(params, dtrain, valid_sets = [dvalid], verbose_eval=args['verbose_eval'], \n",
    "                early_stopping_rounds=args['early_stopping_rounds'])\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = X_train.columns\n",
    "\n",
    "feature_importances[f'final_train'] = clf.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_valid)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_valid, y_pred)\n",
    "\n",
    "reports = precision_reall_f1_report(precision, recall, thresholds, \n",
    "                                    font_scale=2,\n",
    "                                    linewidth=3,\n",
    "                                    plot=False)\n",
    "\n",
    "threshold = reports[reports['f1']==reports['f1'].max()]['threshold'].values[0]\n",
    "\n",
    "print('Threshold to get the best F1 on validation set: ', threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(f\">>> AUC on Test set: {roc_auc_score(y_test, y_pred)}\\n\")\n",
    "\n",
    "y_pred_label = [1 if i >= threshold else 0 for i in y_pred]\n",
    "\n",
    "print(f\">>> F1 on Test set (threshold {threshold}) : {f1_score(y_test, y_pred_label)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1 vs threshold on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = precision_reall_f1_report(precision, recall, thresholds, \n",
    "                                    font_scale=2,\n",
    "                                    linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best F1: ', reports['f1'].max())\n",
    "print('Threshold:', reports[reports['f1']==reports['f1'].max()]['threshold'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_space = pd.read_csv(args['feature_space'], dtype={'feature': str})\n",
    "feature_importances['pattern'] = feature_space['feature'].values\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shap values\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "shap_values = explainer(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.feature_names = feature_importances['pattern'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[:, :, 1], max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
